{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae09e24-6d11-4d1f-a2dd-a5477d6bf6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-03 21:51:49] === Daily ArXiv Data Fetcher Started ===\n",
      "[2025-07-03 21:51:49] üìÅ CSV Path: C:\\arxiv_project\\arxiv_dataset\\articles.csv\n",
      "[2025-07-03 21:51:49] üìÅ Log Path: C:\\arxiv_project\\arxiv_dataset\\daily_fetch.log\n",
      "[2025-07-03 21:51:49] üìÇ CSV Directory: C:\\arxiv_project\\arxiv_dataset\n",
      "[2025-07-03 21:51:49] üìÇ Directory exists: True\n",
      "[2025-07-03 21:52:14] üìÖ Found last date in CSV: 2025-07-02\n",
      "[2025-07-03 21:52:14] üìÖ Last fetch date: 2025-07-02\n",
      "[2025-07-03 21:52:14] üìÖ Today: 2025-07-03\n",
      "[2025-07-03 21:52:14] üìÖ Yesterday: 2025-07-02\n",
      "[2025-07-03 21:52:14] ‚úÖ Already up to date. Last fetch: 2025-07-02\n",
      "[2025-07-03 21:52:14] üß™ For testing, let's try fetching yesterday's data anyway...\n",
      "[2025-07-03 21:52:14] üîé Fetching data for 2025-07-02\n",
      "[2025-07-03 21:52:15] üìÑ Processing 706 entries (batch starting at 0)\n",
      "[2025-07-03 21:52:20] ‚úÖ Fetched 706 articles for 2025-07-02\n",
      "[2025-07-03 21:52:20] üìù Appended 706 articles to existing CSV\n",
      "[2025-07-03 21:52:20] üß™ Test fetch completed with 706 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
    "BATCH_SIZE = 1000\n",
    "REQUEST_INTERVAL = 4  # Respect rate limiting: 1 request per 3+ seconds\n",
    "CSV_PATH = r\"C:\\arxiv_project\\arxiv_dataset\\articles.csv\"\n",
    "LOG_FILE = r\"C:\\arxiv_project\\arxiv_dataset\\daily_fetch.log\"\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log messages to both console and file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\"\n",
    "    print(log_entry)\n",
    "    \n",
    "    # Ensure log directory exists\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    \n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry + '\\n')\n",
    "\n",
    "def extract_arxiv_data(entry, ns):\n",
    "    \"\"\"Extract all arXiv data from XML entry\"\"\"\n",
    "    try:\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        published = entry.find('atom:published', ns).text\n",
    "        updated = entry.find('atom:updated', ns).text\n",
    "        link = entry.find('atom:id', ns).text\n",
    "        \n",
    "        arxiv_id = link.split('/')[-1] if link else \"\"\n",
    "        \n",
    "        authors = [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)]\n",
    "        authors_str = \"; \".join(authors)  # Join authors for CSV\n",
    "        \n",
    "        categories = [cat.get('term') for cat in entry.findall('atom:category', ns)]\n",
    "        categories_str = \"; \".join(categories)  # Join categories for CSV\n",
    "        \n",
    "        primary_category = \"\"\n",
    "        primary_elem = entry.find('atom:primary_category', ns)\n",
    "        if primary_elem is not None:\n",
    "            primary_category = primary_elem.get('term', '')\n",
    "        \n",
    "        doi = None\n",
    "        doi_elem = entry.find('.//atom:link[@title=\"doi\"]', ns)\n",
    "        if doi_elem is not None:\n",
    "            doi_href = doi_elem.get('href', '')\n",
    "            if 'doi.org' in doi_href:\n",
    "                doi = doi_href.split('doi.org/')[-1]\n",
    "        \n",
    "        journal_ref = None\n",
    "        journal_elem = entry.find('atom:journal_ref', ns)\n",
    "        if journal_elem is not None:\n",
    "            journal_ref = journal_elem.text.strip()\n",
    "        \n",
    "        article = {\n",
    "            \"id\": arxiv_id,\n",
    "            \"title\": title,\n",
    "            \"author\": authors_str,\n",
    "            \"published\": published,\n",
    "            \"updated\": updated,\n",
    "            \"link\": link,\n",
    "            \"summary\": summary,\n",
    "            \"primary_category\": primary_category,\n",
    "            \"category\": categories_str,\n",
    "            \"doi\": doi,\n",
    "            \"journal_ref\": journal_ref,\n",
    "            \"entry\": link,\n",
    "            \"name\": f\"{authors[0] if authors else 'Unknown'} et al. - {title[:50]}...\",\n",
    "            \"comment\": None,\n",
    "            \"fetch_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        return article\n",
    "    except Exception as e:\n",
    "        log_message(f\"‚ùå Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_last_fetch_date():\n",
    "    \"\"\"Get the last fetch date from CSV or log file\"\"\"\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        try:\n",
    "            # Read only the last few rows to get the most recent date\n",
    "            df = pd.read_csv(CSV_PATH, usecols=['published'], nrows=None)\n",
    "            if not df.empty:\n",
    "                # Get the most recent published date\n",
    "                df['published_date'] = pd.to_datetime(df['published']).dt.date\n",
    "                last_date = df['published_date'].max()\n",
    "                log_message(f\"üìÖ Found last date in CSV: {last_date}\")\n",
    "                return last_date\n",
    "        except Exception as e:\n",
    "            log_message(f\"‚ö†Ô∏è Error reading CSV for last date: {e}\")\n",
    "    \n",
    "    # If no CSV exists or error, start from yesterday\n",
    "    fallback_date = (datetime.now() - timedelta(days=1)).date()\n",
    "    log_message(f\"üìÖ No CSV found, using fallback date: {fallback_date}\")\n",
    "    return fallback_date\n",
    "\n",
    "def fetch_daily_data(target_date):\n",
    "    \"\"\"Fetch arXiv data for a specific date\"\"\"\n",
    "    start_str = target_date.strftime(\"%Y%m%d0000\")\n",
    "    end_str = target_date.strftime(\"%Y%m%d2359\")\n",
    "    query = f\"submittedDate:[{start_str} TO {end_str}]\"\n",
    "    \n",
    "    log_message(f\"üîé Fetching data for {target_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': 'ArxivDailyBot/1.0'})\n",
    "    \n",
    "    all_articles = []\n",
    "    start = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start,\n",
    "            \"max_results\": BATCH_SIZE,\n",
    "            \"sortBy\": \"submittedDate\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = session.get(ARXIV_API_URL, params=params, timeout=45)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "            entries = root.findall('atom:entry', ns)\n",
    "            \n",
    "            if not entries:\n",
    "                break\n",
    "            \n",
    "            log_message(f\"üìÑ Processing {len(entries)} entries (batch starting at {start})\")\n",
    "            \n",
    "            for entry in entries:\n",
    "                article = extract_arxiv_data(entry, ns)\n",
    "                if article:\n",
    "                    all_articles.append(article)\n",
    "            \n",
    "            start += BATCH_SIZE\n",
    "            time.sleep(REQUEST_INTERVAL)\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"‚ùå Error fetching data: {e}\")\n",
    "            time.sleep(5)\n",
    "            break\n",
    "    \n",
    "    session.close()\n",
    "    log_message(f\"‚úÖ Fetched {len(all_articles)} articles for {target_date.strftime('%Y-%m-%d')}\")\n",
    "    return all_articles\n",
    "\n",
    "def append_to_csv(articles):\n",
    "    \"\"\"Append new articles to CSV file\"\"\"\n",
    "    if not articles:\n",
    "        log_message(\"‚ÑπÔ∏è No articles to append\")\n",
    "        return\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Check if CSV exists\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        # Append to existing CSV\n",
    "        df.to_csv(CSV_PATH, mode='a', header=False, index=False, encoding='utf-8')\n",
    "        log_message(f\"üìù Appended {len(articles)} articles to existing CSV\")\n",
    "    else:\n",
    "        # Create new CSV with headers\n",
    "        df.to_csv(CSV_PATH, mode='w', header=True, index=False, encoding='utf-8')\n",
    "        log_message(f\"üìù Created new CSV with {len(articles)} articles\")\n",
    "\n",
    "def remove_duplicates_from_csv():\n",
    "    \"\"\"Remove duplicate entries from CSV based on arXiv ID\"\"\"\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        initial_count = len(df)\n",
    "        \n",
    "        # Remove duplicates based on 'id' column (arXiv ID)\n",
    "        df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "        final_count = len(df)\n",
    "        \n",
    "        if initial_count != final_count:\n",
    "            df.to_csv(CSV_PATH, index=False, encoding='utf-8')\n",
    "            log_message(f\"üßπ Removed {initial_count - final_count} duplicate entries\")\n",
    "        else:\n",
    "            log_message(\"‚úÖ No duplicates found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_message(f\"‚ùå Error removing duplicates: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to fetch daily data\"\"\"\n",
    "    log_message(\"=== Daily ArXiv Data Fetcher Started ===\")\n",
    "    log_message(f\"üìÅ CSV Path: {CSV_PATH}\")\n",
    "    log_message(f\"üìÅ Log Path: {LOG_FILE}\")\n",
    "    \n",
    "    try:\n",
    "        # Test directory access\n",
    "        csv_dir = os.path.dirname(CSV_PATH)\n",
    "        log_message(f\"üìÇ CSV Directory: {csv_dir}\")\n",
    "        log_message(f\"üìÇ Directory exists: {os.path.exists(csv_dir)}\")\n",
    "        \n",
    "        if not os.path.exists(csv_dir):\n",
    "            log_message(f\"üîß Creating directory: {csv_dir}\")\n",
    "            os.makedirs(csv_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the last fetch date\n",
    "        last_fetch_date = get_last_fetch_date()\n",
    "        log_message(f\"üìÖ Last fetch date: {last_fetch_date}\")\n",
    "        \n",
    "        # Calculate which dates to fetch\n",
    "        yesterday = (datetime.now() - timedelta(days=1)).date()\n",
    "        today = datetime.now().date()\n",
    "        \n",
    "        log_message(f\"üìÖ Today: {today}\")\n",
    "        log_message(f\"üìÖ Yesterday: {yesterday}\")\n",
    "        \n",
    "        # Determine the date range to fetch\n",
    "        if last_fetch_date >= yesterday:\n",
    "            log_message(f\"‚úÖ Already up to date. Last fetch: {last_fetch_date}\")\n",
    "            log_message(\"üß™ For testing, let's try fetching yesterday's data anyway...\")\n",
    "            articles = fetch_daily_data(yesterday)\n",
    "            if articles:\n",
    "                append_to_csv(articles)\n",
    "                log_message(f\"üß™ Test fetch completed with {len(articles)} articles\")\n",
    "            else:\n",
    "                log_message(\"üß™ No articles found for yesterday\")\n",
    "            return\n",
    "        \n",
    "        # Fetch data for missing dates (from day after last fetch to yesterday)\n",
    "        current_date = last_fetch_date + timedelta(days=1)\n",
    "        total_articles = 0\n",
    "        \n",
    "        log_message(f\"üìÖ Will fetch from {current_date} to {yesterday}\")\n",
    "        \n",
    "        # Check if the date range makes sense\n",
    "        if current_date > yesterday:\n",
    "            log_message(f\"‚ö†Ô∏è No dates to fetch: current_date ({current_date}) > yesterday ({yesterday})\")\n",
    "            log_message(\"üß™ For testing, let's fetch yesterday's data...\")\n",
    "            articles = fetch_daily_data(yesterday)\n",
    "            if articles:\n",
    "                append_to_csv(articles)\n",
    "                total_articles = len(articles)\n",
    "                log_message(f\"üß™ Test fetch completed with {len(articles)} articles\")\n",
    "            else:\n",
    "                log_message(\"üß™ No articles found for yesterday\")\n",
    "        else:\n",
    "            # Normal fetching loop\n",
    "            while current_date <= yesterday:\n",
    "                log_message(f\"üîÑ Processing date: {current_date}\")\n",
    "                articles = fetch_daily_data(current_date)\n",
    "                if articles:\n",
    "                    append_to_csv(articles)\n",
    "                    total_articles += len(articles)\n",
    "                else:\n",
    "                    log_message(f\"‚ÑπÔ∏è No articles found for {current_date}\")\n",
    "                \n",
    "                current_date += timedelta(days=1)\n",
    "                time.sleep(2)  # Small delay between dates\n",
    "        \n",
    "        # Remove duplicates\n",
    "        remove_duplicates_from_csv()\n",
    "        \n",
    "        log_message(f\"üéâ Daily fetch completed! Total new articles: {total_articles}\")\n",
    "        \n",
    "        # Log CSV statistics\n",
    "        if os.path.exists(CSV_PATH):\n",
    "            df = pd.read_csv(CSV_PATH)\n",
    "            log_message(f\"üìä Total articles in CSV: {len(df):,}\")\n",
    "        else:\n",
    "            log_message(\"‚ùå CSV file was not created!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_message(f\"‚ùå Fatal error: {e}\")\n",
    "        import traceback\n",
    "        log_message(f\"‚ùå Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def test_simple_fetch():\n",
    "    \"\"\"Simple test function to fetch a small amount of recent data\"\"\"\n",
    "    log_message(\"üß™ === SIMPLE TEST MODE ===\")\n",
    "    \n",
    "    # Test with just 10 recent articles\n",
    "    query = \"cat:cs.AI\"  # Computer Science - AI category\n",
    "    \n",
    "    params = {\n",
    "        \"search_query\": query,\n",
    "        \"start\": 0,\n",
    "        \"max_results\": 10,\n",
    "        \"sortBy\": \"submittedDate\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        log_message(\"üîÑ Making test API request...\")\n",
    "        response = requests.get(ARXIV_API_URL, params=params, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        log_message(f\"‚úÖ API Response Status: {response.status_code}\")\n",
    "        log_message(f\"üìÑ Response Length: {len(response.content)} bytes\")\n",
    "        \n",
    "        root = ET.fromstring(response.content)\n",
    "        ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "        entries = root.findall('atom:entry', ns)\n",
    "        \n",
    "        log_message(f\"üìä Found {len(entries)} entries\")\n",
    "        \n",
    "        if entries:\n",
    "            articles = []\n",
    "            for entry in entries:\n",
    "                article = extract_arxiv_data(entry, ns)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "                    log_message(f\"üìÑ Article: {article['title'][:50]}...\")\n",
    "            \n",
    "            if articles:\n",
    "                log_message(f\"üíæ Attempting to save {len(articles)} articles to CSV...\")\n",
    "                append_to_csv(articles)\n",
    "                \n",
    "                # Check if file was created\n",
    "                if os.path.exists(CSV_PATH):\n",
    "                    file_size = os.path.getsize(CSV_PATH)\n",
    "                    log_message(f\"‚úÖ CSV created successfully! Size: {file_size} bytes\")\n",
    "                else:\n",
    "                    log_message(\"‚ùå CSV file was not created!\")\n",
    "            else:\n",
    "                log_message(\"‚ùå No articles extracted from API response\")\n",
    "        else:\n",
    "            log_message(\"‚ùå No entries found in API response\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_message(f\"‚ùå Test failed: {e}\")\n",
    "        import traceback\n",
    "        log_message(f\"‚ùå Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Add command line argument for test mode\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"test\":\n",
    "        test_simple_fetch()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0124cb-5baf-4884-a8aa-b23dfdf318b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
