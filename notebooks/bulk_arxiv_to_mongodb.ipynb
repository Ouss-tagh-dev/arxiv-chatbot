{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df254a-cb19-489d-a025-f5b2b1dd896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from pymongo import MongoClient, errors\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "DB_NAME = \"arxiv\"\n",
    "COLLECTION_NAME = \"articles\"\n",
    "\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
    "BATCH_SIZE = 1000  \n",
    "MAX_THREADS = 2    # Réduit pour respecter rate limiting\n",
    "REQUEST_INTERVAL = 4  # Augmenté pour respecter \"1 requête/3 secondes\"\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "collection.create_index(\"link\", unique=True)\n",
    "collection.create_index(\"published\")\n",
    "\n",
    "def extract_arxiv_data(entry, ns):\n",
    "    \"\"\"Extraction optimisée de toutes les données arXiv\"\"\"\n",
    "    try:\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        published = entry.find('atom:published', ns).text\n",
    "        updated = entry.find('atom:updated', ns).text\n",
    "        link = entry.find('atom:id', ns).text\n",
    "        \n",
    "        arxiv_id = link.split('/')[-1] if link else \"\"\n",
    "        \n",
    "        authors = [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)]\n",
    "   \n",
    "        categories = [cat.get('term') for cat in entry.findall('atom:category', ns)]\n",
    "        primary_category = \"\"\n",
    "        primary_elem = entry.find('atom:primary_category', ns)\n",
    "        if primary_elem is not None:\n",
    "            primary_category = primary_elem.get('term', '')\n",
    "        \n",
    "        doi = None\n",
    "        doi_elem = entry.find('.//atom:link[@title=\"doi\"]', ns)\n",
    "        if doi_elem is not None:\n",
    "            doi_href = doi_elem.get('href', '')\n",
    "            if 'doi.org' in doi_href:\n",
    "                doi = doi_href.split('doi.org/')[-1]\n",
    "        \n",
    "        journal_ref = None\n",
    "        journal_elem = entry.find('atom:journal_ref', ns)\n",
    "        if journal_elem is not None:\n",
    "            journal_ref = journal_elem.text.strip()\n",
    "        \n",
    "        article = {\n",
    "            \"id\": arxiv_id,\n",
    "            \"title\": title,\n",
    "            \"author\": authors,\n",
    "            \"published\": published,\n",
    "            \"updated\": updated,\n",
    "            \"link\": link,\n",
    "            \"summary\": summary,\n",
    "            \"primary_category\": primary_category,\n",
    "            \"category\": categories,\n",
    "            \"doi\": doi,\n",
    "            \"journal_ref\": journal_ref,\n",
    "            \"entry\": link,  # Même que link pour arXiv\n",
    "            \"name\": f\"{authors[0] if authors else 'Unknown'} et al. - {title[:50]}...\",\n",
    "            \"comment\": None  # Peut être ajouté si disponible\n",
    "        }\n",
    "        \n",
    "        return article\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur extraction: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_and_insert_period(start_date, end_date):\n",
    "    start_str = start_date.strftime(\"%Y%m%d0000\") \n",
    "    end_str = end_date.strftime(\"%Y%m%d2359\")\n",
    "    query = f\"submittedDate:[{start_str} TO {end_str}]\"\n",
    "    print(f\"🔎 Thread démarré pour {start_str} ➡️ {end_str}\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': 'ArxivBot/1.0'})\n",
    "    \n",
    "    start = 0\n",
    "    total_inserted = 0\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start,\n",
    "            \"max_results\": BATCH_SIZE,\n",
    "            \"sortBy\": \"submittedDate\"  \n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = session.get(ARXIV_API_URL, params=params, timeout=45)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "            entries = root.findall('atom:entry', ns)\n",
    "            \n",
    "            total_results_elem = root.find('.//opensearch:totalResults', \n",
    "                                         {'opensearch': 'http://a9.com/-/spec/opensearch/1.1/'})\n",
    "            if total_results_elem is not None:\n",
    "                total_results = int(total_results_elem.text)\n",
    "                if total_results > 30000:\n",
    "                    print(f\"⚠️ ATTENTION: {total_results} résultats pour {start_str}, limite 30k atteinte!\")\n",
    "                    print(f\"   Seuls les premiers 30k seront récupérés. Affinez la période.\")\n",
    "            \n",
    "            if not entries:\n",
    "                break\n",
    "\n",
    "            articles = []\n",
    "            for entry in entries:\n",
    "                article = extract_arxiv_data(entry, ns)\n",
    "                if article:\n",
    "                    articles.append(article)\n",
    "\n",
    "            if articles:\n",
    "                try:\n",
    "                    result = collection.insert_many(articles, ordered=False)\n",
    "                    inserted_count = len(result.inserted_ids)\n",
    "                    total_inserted += inserted_count\n",
    "                    print(f\"✅ {inserted_count} articles insérés pour {start_str}\")\n",
    "                except errors.BulkWriteError as bwe:\n",
    "                    inserted = bwe.details.get(\"nInserted\", 0)\n",
    "                    total_inserted += inserted\n",
    "                    print(f\"⚠️ {inserted} nouveaux, doublons ignorés ({start_str})\")\n",
    "\n",
    "            start += BATCH_SIZE\n",
    "            time.sleep(REQUEST_INTERVAL)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur pour {start_str}: {e}\")\n",
    "            time.sleep(5)  # Pause plus longue en cas d'erreur\n",
    "            break\n",
    "    \n",
    "    session.close()\n",
    "    print(f\"📊 Total inséré pour {start_str}: {total_inserted}\")\n",
    "    return total_inserted\n",
    "\n",
    "# Génère des périodes COURTES pour éviter la limite 30k/50k\n",
    "def generate_week_ranges(start_date, end_date):\n",
    "    current = start_date\n",
    "    while current < end_date:\n",
    "        if current.year >= 2020:\n",
    "            period_days = 2\n",
    "        elif current.year >= 2010:\n",
    "            period_days = 5\n",
    "        else:\n",
    "            period_days = 7\n",
    "            \n",
    "        range_end = min(current + timedelta(days=period_days), end_date)\n",
    "        yield (current, range_end)\n",
    "        current = range_end + timedelta(days=1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Importation arXiv OPTIMISÉE ===\")\n",
    "    \n",
    "    start_date = datetime(1991, 1, 1)\n",
    "    end_date = datetime.now()\n",
    "    \n",
    "    print(f\"📅 Période: {start_date.strftime('%Y-%m-%d')} ➡️ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    ranges = list(generate_week_ranges(start_date, end_date))\n",
    "    print(f\"📊 {len(ranges)} périodes à traiter avec {MAX_THREADS} threads\")\n",
    "    print(f\"⚠️ IMPORTANT: Périodes courtes pour éviter les limites arXiv (30k/requête)\")\n",
    "    print(f\"🐌 Rate limiting respecté: 1 requête/{REQUEST_INTERVAL}s par thread\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_articles = 0\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        futures = [executor.submit(fetch_and_insert_period, start, end) for start, end in ranges]\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                count = future.result()\n",
    "                total_articles += count\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Erreur thread: {e}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n🎉 IMPORTATION TERMINÉE!\")\n",
    "    print(f\"📈 {total_articles:,} articles traités\")\n",
    "    print(f\"⏱️ Temps: {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"⚡ Vitesse: {total_articles/(elapsed_time/60):.0f} articles/minute\")\n",
    "    \n",
    "    try:\n",
    "        total_db = collection.count_documents({})\n",
    "        print(f\"💾 Total en base: {total_db:,} articles\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    client.close()\n",
    "    print(\"🔐 Connexion fermée\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
