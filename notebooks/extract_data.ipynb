{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae09e24-6d11-4d1f-a2dd-a5477d6bf6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-04 16:19:09] === Daily ArXiv Data Fetcher Started ===\n",
      "[2025-07-04 16:19:09] ðŸ“ CSV Path: C:\\arxiv_project\\arxiv_dataset\\articles.csv\n",
      "[2025-07-04 16:19:09] ðŸ“ Log Path: C:\\arxiv_project\\arxiv_dataset\\daily_fetch.log\n",
      "[2025-07-04 16:19:09] ðŸ“‚ CSV Directory: C:\\arxiv_project\\arxiv_dataset\n",
      "[2025-07-04 16:19:09] ðŸ“‚ Directory exists: True\n",
      "[2025-07-04 16:19:37] ðŸ“… Found last date in CSV: 2025-07-02\n",
      "[2025-07-04 16:19:37] ðŸ“… Today: 2025-07-04\n",
      "[2025-07-04 16:19:37] ðŸ“… Yesterday: 2025-07-03\n",
      "[2025-07-04 16:19:37] ðŸ”„ Processing date: 2025-07-03\n",
      "[2025-07-04 16:19:37] ðŸ”Ž Fetching data for 2025-07-03\n",
      "[2025-07-04 16:19:56] ðŸ“„ Processing 659 entries (batch starting at 0)\n",
      "[2025-07-04 16:20:05] âœ… Fetched 659 articles for 2025-07-03\n",
      "[2025-07-04 16:20:05] ðŸ“ Appended 659 articles to existing CSV\n",
      "[2025-07-04 16:22:08] ðŸ§¹ Removed 659 duplicate entries\n",
      "[2025-07-04 16:22:09] ðŸŽ‰ Daily fetch completed! Total new articles: 659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS TUF\\AppData\\Local\\Temp\\ipykernel_21776\\2060849862.py:217: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(CSV_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-04 16:23:01] ðŸ“Š Total articles in CSV: 2,244,856\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
    "BATCH_SIZE = 1000\n",
    "REQUEST_INTERVAL = 4  # Respect rate limiting\n",
    "CSV_PATH = r\"C:\\arxiv_project\\data\\raw\\articles.csv\"\n",
    "LOG_FILE = r\"C:\\arxiv_project\\data\\logs\\daily_fetch.log\"\n",
    "\n",
    "def log_message(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] {message}\"\n",
    "    print(log_entry)\n",
    "    os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry + '\\n')\n",
    "\n",
    "def extract_arxiv_data(entry, ns):\n",
    "    try:\n",
    "        title = entry.find('atom:title', ns).text.strip()\n",
    "        summary = entry.find('atom:summary', ns).text.strip()\n",
    "        published = entry.find('atom:published', ns).text\n",
    "        updated = entry.find('atom:updated', ns).text\n",
    "        link = entry.find('atom:id', ns).text\n",
    "\n",
    "        if not re.match(r\"^\\d{4}-\\d{2}-\\d{2}T\", published):\n",
    "            raise ValueError(f\"Invalid published date: {published}\")\n",
    "\n",
    "        arxiv_id = link.split('/')[-1] if link else \"\"\n",
    "        authors = [author.find('atom:name', ns).text for author in entry.findall('atom:author', ns)]\n",
    "        authors_str = \"; \".join(authors)\n",
    "        categories = [cat.get('term') for cat in entry.findall('atom:category', ns)]\n",
    "        categories_str = \"; \".join(categories)\n",
    "\n",
    "        primary_category = \"\"\n",
    "        primary_elem = entry.find('atom:primary_category', ns)\n",
    "        if primary_elem is not None:\n",
    "            primary_category = primary_elem.get('term', '')\n",
    "\n",
    "        doi = None\n",
    "        doi_elem = entry.find('.//atom:link[@title=\"doi\"]', ns)\n",
    "        if doi_elem is not None:\n",
    "            doi_href = doi_elem.get('href', '')\n",
    "            if 'doi.org' in doi_href:\n",
    "                doi = doi_href.split('doi.org/')[-1]\n",
    "\n",
    "        journal_ref = None\n",
    "        journal_elem = entry.find('atom:journal_ref', ns)\n",
    "        if journal_elem is not None:\n",
    "            journal_ref = journal_elem.text.strip()\n",
    "\n",
    "        article = {\n",
    "            \"id\": arxiv_id,\n",
    "            \"title\": title,\n",
    "            \"author\": authors_str,\n",
    "            \"published\": published,\n",
    "            \"updated\": updated,\n",
    "            \"link\": link,\n",
    "            \"summary\": summary,\n",
    "            \"primary_category\": primary_category,\n",
    "            \"category\": categories_str,\n",
    "            \"doi\": doi,\n",
    "            \"journal_ref\": journal_ref,\n",
    "            \"entry\": link,\n",
    "            \"name\": f\"{authors[0] if authors else 'Unknown'} et al. - {title[:50]}...\",\n",
    "            \"comment\": None,\n",
    "            \"fetch_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        return article\n",
    "    except Exception as e:\n",
    "        log_message(f\"âŒ Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_last_fetch_date():\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        try:\n",
    "            df = pd.read_csv(CSV_PATH, usecols=['published'])\n",
    "            df = df[df['published'].astype(str).str.match(r\"^\\d{4}-\\d{2}-\\d{2}T\")]\n",
    "            df['published_date'] = pd.to_datetime(df['published'], errors='coerce').dt.date\n",
    "            df = df.dropna(subset=['published_date'])\n",
    "            last_date = df['published_date'].max()\n",
    "            log_message(f\"ðŸ“… Found last date in CSV: {last_date}\")\n",
    "            return last_date\n",
    "        except Exception as e:\n",
    "            log_message(f\"âš ï¸ Error reading CSV for last date: {e}\")\n",
    "\n",
    "    fallback_date = (datetime.now() - timedelta(days=1)).date()\n",
    "    log_message(f\"ðŸ“… No CSV found, using fallback date: {fallback_date}\")\n",
    "    return fallback_date\n",
    "\n",
    "def fetch_daily_data(target_date):\n",
    "    start_str = target_date.strftime(\"%Y%m%d0000\")\n",
    "    end_str = target_date.strftime(\"%Y%m%d2359\")\n",
    "    query = f\"submittedDate:[{start_str} TO {end_str}]\"\n",
    "\n",
    "    log_message(f\"ðŸ”Ž Fetching data for {target_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'User-Agent': 'ArxivDailyBot/1.0'})\n",
    "    all_articles, start = [], 0\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start,\n",
    "            \"max_results\": BATCH_SIZE,\n",
    "            \"sortBy\": \"submittedDate\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.get(ARXIV_API_URL, params=params, timeout=45)\n",
    "            response.raise_for_status()\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
    "            entries = root.findall('atom:entry', ns)\n",
    "            if not entries:\n",
    "                break\n",
    "            log_message(f\"ðŸ“„ Processing {len(entries)} entries (batch starting at {start})\")\n",
    "            for entry in entries:\n",
    "                article = extract_arxiv_data(entry, ns)\n",
    "                if article:\n",
    "                    all_articles.append(article)\n",
    "            start += BATCH_SIZE\n",
    "            time.sleep(REQUEST_INTERVAL)\n",
    "        except Exception as e:\n",
    "            log_message(f\"âŒ Error fetching data: {e}\")\n",
    "            time.sleep(5)\n",
    "            break\n",
    "\n",
    "    session.close()\n",
    "    log_message(f\"âœ… Fetched {len(all_articles)} articles for {target_date.strftime('%Y-%m-%d')}\")\n",
    "    return all_articles\n",
    "\n",
    "def append_to_csv(articles):\n",
    "    if not articles:\n",
    "        log_message(\"â„¹ï¸ No articles to append\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)\n",
    "    df = pd.DataFrame(articles)\n",
    "    df = df[df['published'].astype(str).str.match(r\"^\\d{4}-\\d{2}-\\d{2}T\")]\n",
    "\n",
    "    if os.path.exists(CSV_PATH):\n",
    "        df.to_csv(CSV_PATH, mode='a', header=False, index=False, encoding='utf-8')\n",
    "        log_message(f\"ðŸ“ Appended {len(df)} articles to existing CSV\")\n",
    "    else:\n",
    "        df.to_csv(CSV_PATH, mode='w', header=True, index=False, encoding='utf-8')\n",
    "        log_message(f\"ðŸ“ Created new CSV with {len(df)} articles\")\n",
    "\n",
    "def remove_duplicates_from_csv():\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(CSV_PATH, dtype=str, low_memory=False)\n",
    "        initial_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "        final_count = len(df)\n",
    "        if initial_count != final_count:\n",
    "            df.to_csv(CSV_PATH, index=False, encoding='utf-8')\n",
    "            log_message(f\"ðŸ§¹ Removed {initial_count - final_count} duplicate entries\")\n",
    "        else:\n",
    "            log_message(\"âœ… No duplicates found\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"âŒ Error removing duplicates: {e}\")\n",
    "\n",
    "def main():\n",
    "    log_message(\"=== Daily ArXiv Data Fetcher Started ===\")\n",
    "    log_message(f\"ðŸ“ CSV Path: {CSV_PATH}\")\n",
    "    log_message(f\"ðŸ“ Log Path: {LOG_FILE}\")\n",
    "\n",
    "    try:\n",
    "        csv_dir = os.path.dirname(CSV_PATH)\n",
    "        log_message(f\"ðŸ“‚ CSV Directory: {csv_dir}\")\n",
    "        log_message(f\"ðŸ“‚ Directory exists: {os.path.exists(csv_dir)}\")\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "        last_fetch_date = get_last_fetch_date()\n",
    "        yesterday = (datetime.now() - timedelta(days=1)).date()\n",
    "        today = datetime.now().date()\n",
    "        log_message(f\"ðŸ“… Today: {today}\")\n",
    "        log_message(f\"ðŸ“… Yesterday: {yesterday}\")\n",
    "\n",
    "        if last_fetch_date >= yesterday:\n",
    "            log_message(f\"âœ… Already up to date. Last fetch: {last_fetch_date}\")\n",
    "            log_message(\"ðŸ§ª For testing, let's try fetching yesterday's data anyway...\")\n",
    "            articles = fetch_daily_data(yesterday)\n",
    "            if articles:\n",
    "                append_to_csv(articles)\n",
    "                log_message(f\"ðŸ§ª Test fetch completed with {len(articles)} articles\")\n",
    "            return\n",
    "\n",
    "        current_date = last_fetch_date + timedelta(days=1)\n",
    "        total_articles = 0\n",
    "\n",
    "        while current_date <= yesterday:\n",
    "            log_message(f\"ðŸ”„ Processing date: {current_date}\")\n",
    "            articles = fetch_daily_data(current_date)\n",
    "            if articles:\n",
    "                append_to_csv(articles)\n",
    "                total_articles += len(articles)\n",
    "            else:\n",
    "                log_message(f\"â„¹ï¸ No articles found for {current_date}\")\n",
    "            current_date += timedelta(days=1)\n",
    "            time.sleep(2)\n",
    "\n",
    "        remove_duplicates_from_csv()\n",
    "        log_message(f\"ðŸŽ‰ Daily fetch completed! Total new articles: {total_articles}\")\n",
    "\n",
    "        if os.path.exists(CSV_PATH):\n",
    "            df = pd.read_csv(CSV_PATH, dtype=str, low_memory=False)\n",
    "            log_message(f\"ðŸ“Š Total articles in CSV: {len(df):,}\")\n",
    "    except Exception as e:\n",
    "        log_message(f\"âŒ Fatal error: {e}\")\n",
    "        import traceback\n",
    "        log_message(f\"âŒ Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"test\":\n",
    "        test_simple_fetch()\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0124cb-5baf-4884-a8aa-b23dfdf318b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
